#  Clasificaci√≥n de Hongos: Predicci√≥n de Comestibilidad con Naive Bayes

## **Descripci√≥n del problema**
Los hongos en general son poco intuitivos de reconocer y clasificar, por lo que una clasificaci√≥n manual podr√≠a generar errores y terminar en intoxicaciones. En este proyecto abordamos la oportunidad de crear una clasificaci√≥n con Naive Bayes autom√°tica. Proponemos un modelo predictivo que, utilizando atributos descriptivos de los hongos, clasifique cada esp√©cimen como comestible o venenoso.

## **Descripci√≥n del dataset**
El an√°lisis se basa en el conjunto de datos "Mushroom Edibility Classification", obtenido de la plataforma Kaggle. En este dataset encontramos datos descriptivos de distintos hongos como lo son, caracter√≠sticas f√≠sicas, textura, sombrero, color, etc.

## **Significado de las columnas del Dataset**
El dataset consta de una variable objetivo y m√∫ltiples atributos descriptivos, todos ellos de naturaleza categ√≥rica. A continuaci√≥n, se detalla el significado de cada variable y sus posibles valores:


### Variable objetivo:

**class:** <br>
e = Edible<br>
p = Poisonous

### Atributos descriptivos:<br>
**cap-shape: Forma del sombrero**<br>
b = bell (campana)<br>
c = conical (c√≥nico)<br>
x = convex (convexo)<br>
f = flat (plano)<br>
k = knobbed (abultado)<br>
s = sunken (hundido)

**cap-surface: Textura de la superficie del sombrero**<br>
f = fibrous (fibroso)<br>
g = grooves (acanalado)<br>
y = scaly (escamoso)<br>
s = smooth (liso)

**cap-color: Color del sombrero**<br>
n = brown (marr√≥n)<br>
b = buff (beige)<br>
c = cinnamon (canela)<br>
g = gray (gris)<br>
r = green (verde)<br>
p = pink (rosado)<br>
u = purple (p√∫rpura)<br>
e = red (rojo)<br>
w = white (blanco)<br>
y = yellow (amarillo)

**bruises: Moretones al manipular**<br>
t = bruises (s√≠)<br>
f = no bruises (no)<br>

**odor: Olor del hongo**<br>
a = almond (almendra)<br>
l = anise (an√≠s)<br>
c = creosote (creosota)<br>
y = fishy (olor a pescado)<br>
f = foul (desagradable)<br>
m = musty (moho)<br>
n = none (sin olor)<br>
p = pungent (picante/penetrante)<br>
s = spicy (especiado)<br>

**gill-attachment: Uni√≥n de las l√°minas al tallo**<br>
a = attached (unidas)<br>
d = descending (descendentes)<br>
f = free (libres)<br>
n = notched (con muescas)

**gill-spacing: Espaciado entre las l√°minas**<br>
c = close (cercanas)<br>
w = crowded (muy juntas)<br>
d = distant (separadas)<br>

**gill-size: Tama√±o de las l√°minas**<br>
b = broad (anchas)<br>
n = narrow (estrechas)

**gill-color: Color de las l√°minas**<br>
k = black (negro)<br>
n = brown (marr√≥n)<br>
b = buff (beige)<br>
h = chocolate<br>
g = gray (gris)<br>
r = green (verde)<br>
o = orange (naranja)<br>
p = pink (rosado)<br>
u = purple (p√∫rpura)<br>
e = red (rojo)<br>
w = white (blanco)<br>
y = yellow (amarillo)

**stalk-shape: Forma del tallo**<br>
e = enlarging (engrosado)<br>
t = tapering (estrech√°ndose)

**stalk-root: Tipo de ra√≠z del tallo**<br>
b = bulbous (bulbosa)<br>
c = club (de forma de maza)<br>
u = cup (en forma de copa)<br>
e = equal (uniforme)<br>
z = rhizomorphs (rizomorfos)<br>
r = rooted (enraizado)<br>
? = missing (dato faltante)

**stalk-surface-above-ring: Superficie del tallo por encima del anillo**<br>
f = fibrous (fibroso)<br>
y = scaly (escamoso)<br>
k = silky (sedoso)<br>
s = smooth (liso)

**stalk-surface-below-ring: Superficie del tallo por debajo del anillo**<br>
Mismos valores que stalk-surface-above-ring

**stalk-color-above-ring: Color del tallo por encima del anillo**<br>
n = brown (marr√≥n)<br>
b = buff (beige)<br>
c = cinnamon (canela)<br>
g = gray (gris)<br>
o = orange (naranja)<br>
p = pink (rosado)<br>
e = red (rojo)<br>
w = white (blanco)<br>
y = yellow (amarillo)

**stalk-color-below-ring: Color del tallo por debajo del anillo**<br>
Mismos valores que stalk-color-above-ring

**veil-type: Tipo de velo (protecci√≥n inicial del hongo)**<br>
p = partial (parcial)<br>
u = universal (completo)

**veil-color: Color del velo**<br>
n = brown (marr√≥n)<br>
o = orange (naranja)<br>
w = white (blanco)<br>
y = yellow (amarillo)

**ring-number: N√∫mero de anillos en el tallo**<br>
n = none (ninguno)<br>
o = one (uno)<br>
t = two (dos)

**ring-type: Tipo de anillo en el tallo**<br>
c = cobwebby (tipo telara√±a)<br>
e = evanescent (ef√≠mero)<br>
f = flaring (acampanado)<br>
l = large (grande)<br>
n = none (ninguno)<br>
p = pendant (colgante)<br>
s = sheathing (envolvente)<br>
z = zone (en zonas)

**spore-print-color: Color de la impresi√≥n de esporas**<br>
k = black (negro)<br>
n = brown (marr√≥n)<br>
b = buff (beige)<br>
h = chocolate<br>
r = green (verde)<br>
o = orange (naranja)<br>
u = purple (p√∫rpura)<br>
w = white (blanco)<br>
y = yellow (amarillo)

**population: Densidad de la poblaci√≥n de hongos en el √°rea**<br>
a = abundant (abundante)<br>
c = clustered (agrupados)<br>
n = numerous (numerosos)<br>
s = scattered (dispersos)<br>
v = several (varios)<br>
y = solitary (solitarios)

**habitat: H√°bitat donde se encuentran los hongos**<br>
g = grasses (pastizales)<br>
l = leaves (hojarasca)<br>
m = meadows (praderas)<br>
p = paths (caminos)<br>
u = urban (urbano)<br>
w = waste (√°reas de desecho)<br>
d = woods (bosques)

## 4. Justificaci√≥n de los modelos seleccionados

Se decidi√≥ implementar y comparar dos algoritmos de clasificaci√≥n para evaluar cu√°l ofrece mejor rendimiento en la clasificaci√≥n de hongos: **Naive Bayes** y **Random Forest**.

### Naive Bayes (CategoricalNB)

Se eligi√≥ el algoritmo Naive Bayes, en particular el clasificador de tipo multinomial, por los siguientes motivos:

Los atributos son categ√≥ricos y discretos, lo que se ajusta perfectamente al supuesto de Naive Bayes.<br>

Es un modelo simple, r√°pido y efectivo para tareas de clasificaci√≥n binaria.<br>

Basado en el Teorema de Bayes, estima la probabilidad de cada clase asumiendo independencia entre los atributos, lo cual, aunque simplifica la realidad, suele ser suficiente para obtener buenos resultados.<br>

Tiene buen desempe√±o incluso con datasets de tama√±o medio como este.

### Random Forest

La inclusi√≥n de Random Forest como segundo modelo se justifica por las siguientes razones:

Es un algoritmo robusto que maneja bien variables categ√≥ricas sin necesidad de asunciones de independencia entre caracter√≠sticas.<br>

Al ser un m√©todo de ensemble, combina m√∫ltiples √°rboles de decisi√≥n, lo que reduce el riesgo de overfitting y mejora la generalizaci√≥n.<br>

Proporciona informaci√≥n valiosa sobre la importancia de las caracter√≠sticas, permitiendo identificar qu√© atributos son m√°s relevantes para la clasificaci√≥n.<br>

Es menos sensible a outliers y ruido en los datos comparado con modelos m√°s simples.<br>

Su capacidad para capturar interacciones complejas entre variables lo hace especialmente adecuado para problemas donde las relaciones entre caracter√≠sticas pueden ser no lineales.


## Limitaciones:

‚Ä¢ Todos los atributos son categ√≥ricos, lo que limita el uso directo de modelos que requieren datos num√©ricos. Aunque pueden codificarse, esta transformaci√≥n puede introducir ruido, especialmente si se utiliza una codificaci√≥n ordinal para variables sin orden impl√≠cito.<br>

‚Ä¢ Algunos atributos (por ejemplo, odor) tienen una fuerte correlaci√≥n con la clase, mientras que otros pueden ser redundantes o incluso irrelevantes, afectando negativamente a algunos modelos si no se realiza una selecci√≥n adecuada de variables.<br>

‚Ä¢ Los modelos clasificadores solo identifican correlaciones estad√≠sticas. No pueden determinar si un atributo como ‚Äúodor‚Äù causa que un hongo sea venenoso, solo que hay asociaci√≥n.

# Metodolog√≠a aplicada<br>
## Carga y Exploraci√≥n de Datos (EDA)

‚Ä¢ Revisi√≥n de la distribuci√≥n de clases (edible vs poisonous).

‚Ä¢ An√°lisis de los valores de los atributos categ√≥ricos.

‚Ä¢ Visualizaci√≥n de correlaciones entre caracter√≠sticas.

## Preprocesamiento de Datos

‚Ä¢ Codificaci√≥n de variables categ√≥ricas mediante LabelEncoder.

‚Ä¢ Revisi√≥n de datos faltantes y consistencia.

## Divisi√≥n de Datos

‚Ä¢ Separaci√≥n en conjuntos de entrenamiento y prueba (ejemplo: 80% - 20%).

‚Ä¢ No se requiere vectorizaci√≥n de texto, ya que todos los atributos son categ√≥ricos.

## Entrenamiento del Modelo

‚Ä¢ Implementaci√≥n de Naive Bayes con CategoricalNB y Random Forest con RandomForestClassifier de scikit-learn.

## Evaluaci√≥n y Visualizaci√≥n de Resultados

‚Ä¢ Matriz de confusi√≥n.

‚Ä¢ M√©tricas de clasificaci√≥n: Accuracy, Precision, Recall, F1-score.

‚Ä¢ Discusi√≥n de hallazgos y posibles mejoras.

## Control de Overfitting

‚Ä¢ Validaci√≥n cruzada.

‚Ä¢ Comparaci√≥n de m√©tricas entre entrenamiento y prueba.

## Resultados y Conclusiones

### Comparaci√≥n de Modelos: Naive Bayes vs Random Forest

Despu√©s de entrenar y evaluar ambos modelos en el dataset de hongos, se obtuvieron los siguientes resultados comparativos:

#### **Datos del Dataset:**
- **Tama√±o total:** 8,124 hongos con 23 caracter√≠sticas
- **Distribuci√≥n de clases:** 
  - Comestibles (e): 4,208 (51.8%)
  - Venenosos (p): 3,916 (48.2%)
- **Divisi√≥n:** 80% entrenamiento (6,499) / 20% prueba (1,625)

#### **Rendimiento Comparativo:**

| M√©trica | Naive Bayes | Random Forest |
|---------|-------------|---------------|
| **Accuracy (Test Set)** | 95.1% | 100.0% |
| **Accuracy (Validaci√≥n Cruzada)** | 95.3% | 100.0% |
| **Desviaci√≥n Est√°ndar** | 0.7% | 0.0% |

#### **An√°lisis de Errores Cr√≠ticos:**

| Tipo de Error | Naive Bayes | Random Forest |
|---------------|-------------|---------------|
| **Falsos Positivos** | 74 casos | 0 casos |
| **Falsos Negativos** | 6 casos | 0 casos |

**Nota:** Los falsos positivos (hongos venenosos clasificados como comestibles) representan el error m√°s cr√≠tico en este contexto.

#### **M√©tricas Detalladas por Modelo:**

**Naive Bayes:**
- **Hongos Comestibles:** Precisi√≥n 92%, Recall 99%, F1-score 95%
- **Hongos Venenosos:** Precisi√≥n 99%, Recall 91%, F1-score 95%

**Random Forest:**
- **Hongos Comestibles:** Precisi√≥n 100%, Recall 100%, F1-score 100%
- **Hongos Venenosos:** Precisi√≥n 100%, Recall 100%, F1-score 100%

### Conclusiones Principales

#### **Rendimiento Superior de Random Forest:**

1. **Precisi√≥n Perfecta:** Random Forest logr√≥ un 100% de precisi√≥n tanto en el conjunto de prueba como en validaci√≥n cruzada, superando significativamente a Naive Bayes.

2. **Eliminaci√≥n de Errores Cr√≠ticos:** Random Forest no present√≥ falsos positivos ni falsos negativos, eliminando completamente el riesgo de clasificar hongos venenosos como comestibles.

3. **Estabilidad Absoluta:** La desviaci√≥n est√°ndar de 0.0% en validaci√≥n cruzada indica consistencia perfecta entre diferentes particiones de datos.

#### **Desempe√±o de Naive Bayes:**

1. **Rendimiento S√≥lido:** Con 95.3% de precisi√≥n promedio, Naive Bayes demostr√≥ ser un clasificador competente.

2. **Errores Controlados:** Aunque present√≥ 74 falsos positivos, mantuvo un n√∫mero relativamente bajo considerando el tama√±o del dataset.

3. **Eficiencia Computacional:** Ofrece entrenamiento m√°s r√°pido y menor complejidad computacional.

#### **Consideraciones Pr√°cticas:**

**Recomendaci√≥n:** Random Forest es el modelo recomendado para esta aplicaci√≥n debido a su precisi√≥n perfecta y eliminaci√≥n total de errores cr√≠ticos.

**Ventajas de Random Forest:**
- Precisi√≥n perfecta en la clasificaci√≥n
- Eliminaci√≥n completa de riesgos de seguridad
- Capacidad de identificar caracter√≠sticas m√°s importantes
- Mayor robustez ante variaciones en los datos

**Ventajas de Naive Bayes:**
- Menor tiempo de entrenamiento
- Mayor simplicidad e interpretabilidad
- Menor requerimiento computacional
- Buen rendimiento general con recursos limitados

### Conclusi√≥n Final

El an√°lisis comparativo demuestra la superioridad clara de Random Forest sobre Naive Bayes para la clasificaci√≥n de hongos comestibles vs venenosos. Con una **precisi√≥n perfecta del 100%** y la eliminaci√≥n total de errores cr√≠ticos, Random Forest proporciona el nivel de seguridad requerido para una aplicaci√≥n tan sensible.

Aunque Naive Bayes mostr√≥ un rendimiento s√≥lido (95.3%), la presencia de 74 falsos positivos representa un riesgo inaceptable en el contexto de seguridad alimentaria. 

**Random Forest se establece como el modelo de elecci√≥n** para este problema, ofreciendo la confiabilidad necesaria para una herramienta de apoyo en la identificaci√≥n de hongos, manteniendo siempre la recomendaci√≥n de que debe complementarse con conocimiento experto humano.

##  Estructura del repositorio

     PROYECTO-INTRO-IA-/
    - proyecto_IA.ipynb       # Notebook principal del proyecto 
    - dataset/                        # Carpeta con el dataset
        -mushrooms.csv
    - requirements.yml                # Dependencias del proyecto 
    - README.md                       # Este archivo explicativo

## Librer√≠as principales:

    - numpy
    - pandas
    - scikit-learn
    - seaborn
    - matplotlib

üîó Fuente del Dataset
Kaggle - Mushroom Classification Dataset
https://www.kaggle.com/datasets/uciml/mushroom-classification